# -*- coding: utf-8 -*-
"""final project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xH-EWevvpQ5gOU52sTQ5_KipMBcwXPLA

# **Step-by-Step Code for Data Preprocessing**

**Step 1: Data Collection and Loading the Dataset: First, let’s load the Gold price regression dataset and take an initial look at the data**
"""

from google.colab import drive
drive.mount('/content/drive')

# Install necessary libraries
!pip install numpy -q
!pip install pandas -q
!pip install matplotlib -q
!pip install seaborn -q
!pip install scipy  -q
!pip install scikit-learn -q

# Step 1: Import all Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/final project/gold_price_data.csv')

# Check the shape of the dataset (rows, columns)
print(f"Shape:\n", df.shape)

# Rename the column
df.rename(columns={'Glod':'Gold'}, inplace=True)

#Check the columns
print(f"Column Names:\n",df.columns)

# Preview the first 5 rows of the dataset
print(f"\n",df.head(5))

# Check for basic info like data types and missing values
print(f"\n Basic Info of Dataset:\n",df.info())

# Summary statistics for numerical columns
print(df.describe())

"""**Feature Eng: Converting Categorical Data into Numerical and extracting useful features for our model**"""

# Convert 'Date' column to datetime objects
df['Date'] = pd.to_datetime(df['Date'])

# Extract useful features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# Drop the original 'Date' column
df = df.drop(columns=['Date'])

print(df.head())

"""**Handling missing values**"""

# Check for missing values in each column
missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)

"""**Exploratory Data Analysis (EDA)**"""

# Set style for the plots
sns.set(style="whitegrid")

# Plot the distribution of the Gold price
plt.figure(figsize=(10, 5))
sns.histplot(df['Gold'], kde=True, bins=30)
plt.title('Distribution of Gold Prices')
plt.xlabel('Gold Price')
plt.ylabel('Frequency')
plt.savefig('gold_price_distribution.png', dpi=300)  # Save figure
plt.show()

# Time series plot of Gold prices over the years
plt.figure(figsize=(14, 6))
sns.lineplot(data=df, x='Year', y='Gold', hue='Month', palette='tab20', legend='full')
plt.title('Gold Prices Over the Years (Color-coded by Month)')
plt.xlabel('Year')
plt.ylabel('Gold Price')
plt.savefig('gold_prices_over_years.png', dpi=300)  # Save figure
plt.show()

# Pairplot to see the relationship between key variables
sns.pairplot(df[['Gold', 'SPX', 'USO', 'SLV', 'EUR/USD']], diag_kind='kde')
plt.savefig('pairplot_gold_spx_uso_slv_eurusd.png', dpi=300)  # Save figure
plt.show()

# Plot histograms for key variables
plt.figure(figsize=(14, 8))

# Histogram for Gold prices
plt.subplot(2, 2, 1)
sns.histplot(df['Gold'], kde=True, bins=30, color='gold')
plt.title('Histogram of Gold Prices')
plt.xlabel('Gold Price')
plt.ylabel('Frequency')

# Histogram for SPX (S&P 500 Index)
plt.subplot(2, 2, 2)
sns.histplot(df['SPX'], kde=True, bins=30, color='skyblue')
plt.title('Histogram of SPX Index')
plt.xlabel('SPX Value')
plt.ylabel('Frequency')

# Histogram for USO (Oil Fund)
plt.subplot(2, 2, 3)
sns.histplot(df['USO'], kde=True, bins=30, color='green')
plt.title('Histogram of USO')
plt.xlabel('USO Value')
plt.ylabel('Frequency')

# Histogram for SLV (Silver Price)
plt.subplot(2, 2, 4)
sns.histplot(df['SLV'], kde=True, bins=30, color='silver')
plt.title('Histogram of SLV')
plt.xlabel('SLV Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.savefig('histograms_gold_spx_uso_slv.png', dpi=300)  # Save figure
plt.show()

"""**Correlation Analysis**"""

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Plot the heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.savefig('correlation_matrix.png', dpi=300)  # Save figure
plt.show()

"""# **Hyperparameter Tuning with Grid SearchCV for Each Regression Model**

**spliting the data**
"""

# Separate features (X) and target (y)
X = df.drop('Gold', axis=1)
y = df['Gold']

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the training and testing sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""**Scaling**"""

# Columns to scale: exclude 'Year', 'Month', 'Day' if you think they shouldn't be scaled
columns_to_scale = ['SPX', 'USO', 'SLV', 'EUR/USD']

# Initialize the scaler
scaler = StandardScaler()

# Scale selected columns in the training and testing data
X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])
X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])

# Check the updated data
print("Sample of scaled X_train:\n", X_train.head())

"""**Linear Regression**"""

# Initialize and train the Linear Regression model
linear_regressor = LinearRegression()

linear_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred_linear = linear_regressor.predict(X_test)

# Evaluate the Linear Regression model
mse_linear = mean_squared_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)

print("Linear Regression - Mean Squared Error:", mse_linear)
print("Linear Regression - R-squared:", r2_linear)

"""**Ridge Regression**"""

# Define the model
ridge_regressor = Ridge()

# Define the parameter grid for extended tuning
param_grid = {
    'alpha': [0.01, 0.1, 1, 10, 100],
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'],
    'max_iter': [1000, 2000, 5000],
    'tol': [1e-3, 1e-4, 1e-5]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=ridge_regressor, param_grid=param_grid, cv=5, scoring='r2')

# Fit the model on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and train the final model
best_ridge = grid_search.best_estimator_
print("Best parameters for Ridge Regression:", grid_search.best_params_)

# Make predictions on the test set
y_pred_ridge = best_ridge.predict(X_test)

# Evaluate the model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

print("Ridge Regression - Mean Squared Error:", mse_ridge)
print("Ridge Regression - R-squared:", r2_ridge)

"""**1. Support Vector Regression (SVR)**"""

# Define the model
svr = SVR()

# Set the hyperparameters and their values for tuning
svr_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Create a GridSearchCV object
svr_grid_search = GridSearchCV(estimator=svr, param_grid=svr_param_grid, cv=5)

# Fit the model on the training data
svr_grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_svr_params = svr_grid_search.best_params_
print("Best SVR Hyperparameters: ", best_svr_params)

# Evaluate the best model
best_svr_model = svr_grid_search.best_estimator_
y_pred_svr = best_svr_model.predict(X_test)

# Calculate performance metrics
mse_svr = mean_squared_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)

print("SVR Mean Squared Error: ", mse_svr)
print("SVR R² Score: ", r2_svr)

"""**2. Decision Tree Regression**"""

# Define the model
dt = DecisionTreeRegressor()

# Set the hyperparameters and their values for tuning
dt_param_grid = {
    'max_depth': [None, 2, 4, 6, 8, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
dt_grid_search = GridSearchCV(estimator=dt, param_grid=dt_param_grid, cv=5)

# Fit the model on the training data
dt_grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_dt_params = dt_grid_search.best_params_
print("Best Decision Tree Hyperparameters: ", best_dt_params)

# Evaluate the best model
best_dt_model = dt_grid_search.best_estimator_
y_pred_dt = best_dt_model.predict(X_test)

# Calculate performance metrics
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print("Decision Tree Mean Squared Error: ", mse_dt)
print("Decision Tree R² Score: ", r2_dt)

"""**3. Random Forest Regression**"""

# Define the model
rf = RandomForestRegressor()

# Set the hyperparameters and their values for tuning
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=5)

# Fit the model on the training data
rf_grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_rf_params = rf_grid_search.best_params_
print("Best Random Forest Hyperparameters: ", best_rf_params)

# Evaluate the best model
best_rf_model = rf_grid_search.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)

# Calculate performance metrics
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("Random Forest Mean Squared Error: ", mse_rf)
print("Random Forest R² Score: ", r2_rf)